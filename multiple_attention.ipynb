{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKh8ztfYwyExQRzpVukXaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ainsley-snell/Data_Mining_CS290/blob/main/multiple_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "P5THoYqe2Ez5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in=5\n",
        "d_out=9\n",
        "num_heads=3\n",
        "dropout=0\n",
        "context_length=3\n",
        "batch_size=2\n",
        "qkv_bias= False\n",
        "head_dim = d_out // num_heads\n",
        "\n",
        "# Creating input batch\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89, 0.73, 0.12],\n",
        "   [0.55, 0.87, 0.66, 0.65, 0.67],\n",
        "   [0.57, 0.85, 0.64, 0.32, 0.13]]\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "b, num_tokens, d_in = batch.shape\n",
        "x= batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVsjVgSg3Vhu",
        "outputId": "f36dba3b-13eb-45d9-d4f6-c527b58617c0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear prjections: transforming each token into query, key, and value vectors\n",
        "\n",
        "\n",
        "# This creates linear layers that change tokens from d_in to d_out\n",
        "W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "W_value = nn.Linear(d_in, d_out, bias=qkv_bias)"
      ],
      "metadata": {
        "id": "xlD_2NC13LhW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes linear layers just created and applies them to each token\n",
        "# creating three new vectors per token which are all size of d_out\n",
        "\n",
        "queries = W_query(x)\n",
        "keys = W_key(x)\n",
        "values = W_value(x)\n",
        "\n",
        "queries.shape\n",
        "queries\n",
        "keys.shape\n",
        "keys\n",
        "values.shape\n",
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eQKs07P6zUl",
        "outputId": "6b0e7d28-b1a3-482e-d88f-9dfaec039ba5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1009, -0.1575,  0.5585, -0.2063,  0.1611, -0.2919,  0.0823,\n",
              "           0.3024,  0.0795],\n",
              "         [-0.0973, -0.2511,  0.0275, -0.0710,  0.0127, -0.0363,  0.0175,\n",
              "           0.2394,  0.2842],\n",
              "         [ 0.0160, -0.3157,  0.0161, -0.0586,  0.1638, -0.0202, -0.1174,\n",
              "           0.2069,  0.1298]],\n",
              "\n",
              "        [[-0.1009, -0.1575,  0.5585, -0.2063,  0.1611, -0.2919,  0.0823,\n",
              "           0.3024,  0.0795],\n",
              "         [-0.0973, -0.2511,  0.0275, -0.0710,  0.0127, -0.0363,  0.0175,\n",
              "           0.2394,  0.2842],\n",
              "         [ 0.0160, -0.3157,  0.0161, -0.0586,  0.1638, -0.0202, -0.1174,\n",
              "           0.2069,  0.1298]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into multiple heads or mutliple attentions\n",
        "# The d_out is split so there are multiple heads, in which the model can look at different parts of the input in parallel\n",
        "queries = queries.view(batch_size, context_length, num_heads, head_dim)\n",
        "keys = keys.view(batch_size, context_length, num_heads, head_dim)\n",
        "values = values.view(batch_size, context_length, num_heads, head_dim)\n",
        "queries.shape\n",
        "queries\n",
        "keys.shape\n",
        "keys\n",
        "values.shape\n",
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHj_NMuX7tw_",
        "outputId": "f1c8047f-9425-4724-c9c2-13da64a8224a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.1009, -0.1575,  0.5585],\n",
              "          [-0.2063,  0.1611, -0.2919],\n",
              "          [ 0.0823,  0.3024,  0.0795]],\n",
              "\n",
              "         [[-0.0973, -0.2511,  0.0275],\n",
              "          [-0.0710,  0.0127, -0.0363],\n",
              "          [ 0.0175,  0.2394,  0.2842]],\n",
              "\n",
              "         [[ 0.0160, -0.3157,  0.0161],\n",
              "          [-0.0586,  0.1638, -0.0202],\n",
              "          [-0.1174,  0.2069,  0.1298]]],\n",
              "\n",
              "\n",
              "        [[[-0.1009, -0.1575,  0.5585],\n",
              "          [-0.2063,  0.1611, -0.2919],\n",
              "          [ 0.0823,  0.3024,  0.0795]],\n",
              "\n",
              "         [[-0.0973, -0.2511,  0.0275],\n",
              "          [-0.0710,  0.0127, -0.0363],\n",
              "          [ 0.0175,  0.2394,  0.2842]],\n",
              "\n",
              "         [[ 0.0160, -0.3157,  0.0161],\n",
              "          [-0.0586,  0.1638, -0.0202],\n",
              "          [-0.1174,  0.2069,  0.1298]]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposing the query, key, and value tensors\n",
        "# Swaps the second and third dimensions so matrices can be multiplied\n",
        "queries = queries.transpose(1, 2)\n",
        "keys = keys.transpose(1, 2)\n",
        "values = values.transpose(1, 2)\n",
        "\n",
        "queries.shape\n",
        "queries\n",
        "keys.shape\n",
        "keys\n",
        "values.shape\n",
        "values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtRWB1Gv7141",
        "outputId": "f58eb09d-e6c9-4dc4-ed76-507f863335db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating attention scores with dot product of queries and keys for each head\n",
        "attn_scores = queries @ keys.transpose(2, 3)\n",
        "attn_scores.shape\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK0ohPq88DE6",
        "outputId": "cad34cf7-1896-4029-eaf8-4b5899e700b8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.0337,  0.1558,  0.2383],\n",
              "          [-0.1445,  0.0005, -0.1732],\n",
              "          [ 0.2203, -0.1687,  0.1164]],\n",
              "\n",
              "         [[ 0.3503, -0.0333,  0.3555],\n",
              "          [-0.3120,  0.0748, -0.3921],\n",
              "          [-0.0432, -0.1391,  0.3092]],\n",
              "\n",
              "         [[ 0.1931, -0.0354,  0.2057],\n",
              "          [-0.1632,  0.0893, -0.1318],\n",
              "          [-0.0037,  0.0610,  0.1624]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0337,  0.1558,  0.2383],\n",
              "          [-0.1445,  0.0005, -0.1732],\n",
              "          [ 0.2203, -0.1687,  0.1164]],\n",
              "\n",
              "         [[ 0.3503, -0.0333,  0.3555],\n",
              "          [-0.3120,  0.0748, -0.3921],\n",
              "          [-0.0432, -0.1391,  0.3092]],\n",
              "\n",
              "         [[ 0.1931, -0.0354,  0.2057],\n",
              "          [-0.1632,  0.0893, -0.1318],\n",
              "          [-0.0037,  0.0610,  0.1624]]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a mask so tokens can't look ahead, it can only look to whats before it\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "mask_bool = mask.bool()[:num_tokens, :num_tokens]"
      ],
      "metadata": {
        "id": "xKCJQwjS8DrQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax\n",
        "# converts attention scores to attention weights\n",
        "# each row sums to 1\n",
        "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "attn_weights.shape\n",
        "attn_weights\n",
        "row_sums = attn_weights.sum(dim=-1)\n",
        "row_sums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC9N8N528GmG",
        "outputId": "e07fb49a-5a40-42c3-e65d-7b5dfbed81cc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000]],\n",
              "\n",
              "        [[1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000]]], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Context vectors\n",
        "# Multiplies attention weights with values to get weighted sum of values for each token in each head\n",
        "# Transposes so that heads can be combined later\n",
        "context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "context_vec.shape\n",
        "context_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DysEtnFP8KXc",
        "outputId": "9a47cd1b-0f16-4317-bbdb-871b26ddbf65"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-0.1009, -0.1575,  0.5585],\n",
              "          [-0.0973, -0.2511,  0.0275],\n",
              "          [ 0.0160, -0.3157,  0.0161]],\n",
              "\n",
              "         [[-0.1558,  0.0084,  0.1155],\n",
              "          [-0.0827, -0.1045, -0.0080],\n",
              "          [-0.0240, -0.0585, -0.0034]],\n",
              "\n",
              "         [[-0.0687,  0.0934,  0.1460],\n",
              "          [-0.0451,  0.0172,  0.1077],\n",
              "          [-0.0555,  0.0262,  0.0440]]],\n",
              "\n",
              "\n",
              "        [[[-0.1009, -0.1575,  0.5585],\n",
              "          [-0.0973, -0.2511,  0.0275],\n",
              "          [ 0.0160, -0.3157,  0.0161]],\n",
              "\n",
              "         [[-0.1558,  0.0084,  0.1155],\n",
              "          [-0.0827, -0.1045, -0.0080],\n",
              "          [-0.0240, -0.0585, -0.0034]],\n",
              "\n",
              "         [[-0.0687,  0.0934,  0.1460],\n",
              "          [-0.0451,  0.0172,  0.1077],\n",
              "          [-0.0555,  0.0262,  0.0440]]]], grad_fn=<TransposeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combines heads back together into a single vector per token\n",
        "context_vec = context_vec.contiguous().view(b, num_tokens, d_out)\n",
        "context_vec.shape\n",
        "context_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0KJxWfl8Nkr",
        "outputId": "3a66387d-8dd1-452e-cc10-407c54d36c11"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1009, -0.1575,  0.5585, -0.0973, -0.2511,  0.0275,  0.0160,\n",
              "          -0.3157,  0.0161],\n",
              "         [-0.1558,  0.0084,  0.1155, -0.0827, -0.1045, -0.0080, -0.0240,\n",
              "          -0.0585, -0.0034],\n",
              "         [-0.0687,  0.0934,  0.1460, -0.0451,  0.0172,  0.1077, -0.0555,\n",
              "           0.0262,  0.0440]],\n",
              "\n",
              "        [[-0.1009, -0.1575,  0.5585, -0.0973, -0.2511,  0.0275,  0.0160,\n",
              "          -0.3157,  0.0161],\n",
              "         [-0.1558,  0.0084,  0.1155, -0.0827, -0.1045, -0.0080, -0.0240,\n",
              "          -0.0585, -0.0034],\n",
              "         [-0.0687,  0.0934,  0.1460, -0.0451,  0.0172,  0.1077, -0.0555,\n",
              "           0.0262,  0.0440]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}