{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMls1+r86fuv8uSw9R2nX8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ainsley-snell/Data_Mining_CS290/blob/main/chapt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTTrQikvZMiY",
        "outputId": "9e0f8948-c611-4865-b03c-9ffe70577825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1212, 318, 616, 3290, 42805]\n",
            "Parameter containing:\n",
            "tensor([[ 0.8255, -0.9730,  0.3501,  0.9520,  0.1073,  1.1137, -1.1786,  0.6746],\n",
            "        [ 0.9046,  0.0460,  0.9300,  0.4690, -0.8641,  0.4575,  1.3406, -0.7128],\n",
            "        [-0.6020, -1.7164, -0.6697, -0.0984, -0.0059, -0.9036,  0.0898, -0.5996],\n",
            "        [ 1.2326, -1.4545, -2.0221,  0.6826,  0.7042,  0.6563, -0.2528,  1.2570]],\n",
            "       requires_grad=True)\n",
            "[0.8254863619804382, -0.9730457067489624, 0.3501078486442566, 0.9519725441932678, 0.10733524709939957, 1.1137068271636963, -1.1786366701126099, 0.6746042370796204]\n",
            "[0.9046087861061096, 0.04602408781647682, 0.9299994707107544, 0.4689948856830597, -0.864142894744873, 0.4574982821941376, 1.340553641319275, -0.7127615809440613]\n",
            "[-0.601959764957428, -1.7164299488067627, -0.6697110533714294, -0.09836871922016144, -0.005870187189429998, -0.9035903811454773, 0.08981180936098099, -0.599642276763916]\n",
            "[1.232577919960022, -1.4545302391052246, -2.02207350730896, 0.6825509667396545, 0.704231858253479, 0.6563321352005005, -0.2528476119041443, 1.257027268409729]\n",
            "tensor([-0.6020, -1.7164, -0.6697, -0.0984, -0.0059, -0.9036,  0.0898, -0.5996])\n",
            "tensor(-0.6722)\n",
            "tensor(-1.1530)\n",
            "tensor(4.9508)\n",
            "tensor(1.6680)\n",
            "tensor([-0.6722, -1.1530,  4.9508,  1.6680])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -2.5221, -10.3229,  -7.9962,  -0.5292,   2.0698,  -4.6549,  -0.7305,\n",
              "         -0.5036])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "raw_text=\"This is my dog Hazel\"\n",
        "tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
        "enc_text= tokenizer.encode(raw_text)\n",
        "print(enc_text)\n",
        "\n",
        "vocab_size= 4\n",
        "output_dim= 8\n",
        "inputs= torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(inputs.weight)\n",
        "\n",
        "inputs= inputs.weight.data\n",
        "inputs\n",
        "\n",
        "inputs.shape\n",
        "for row in inputs:\n",
        "    print(row.tolist())\n",
        "\n",
        "x= torch.Tensor([1.1,2.3])\n",
        "y= torch.Tensor([3.4,-2.1])\n",
        "\n",
        "1.1 * 3.4 + (2.3 * -2.1)\n",
        "\n",
        "torch.dot(x,y)\n",
        "\n",
        "query= inputs[2]\n",
        "print(query)\n",
        "\n",
        "for i in range(len(inputs)):\n",
        "    print(torch.dot(query,inputs[i]))\n",
        "\n",
        "attention_scores_2= torch.zeros(len(inputs))\n",
        "for i in range(len(inputs)):\n",
        "    attention_scores_2[i]=torch.dot(query,inputs[i])\n",
        "print(attention_scores_2)\n",
        "\n",
        "#normalize attention scores using the softmax funtion to become weights:\n",
        "#def softmax(x):\n",
        "#    torch.exp(x)/ torch.exp(x).sum()\n",
        "\n",
        "attention_weights_2= torch.softmax(attention_scores_2, dim=0)\n",
        "attention_weights_2.sum()\n",
        "\n",
        "#use weights to create context vector\n",
        "context_vector_2= torch.zeros(query.shape)\n",
        "for i in range(len(attention_weights_2)) :\n",
        "    context_vector_2 += attention_weights_2[i]* inputs[i]\n",
        "context_vector_2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get all attention scores via matrix multiplication (inputs.T is transposed inputs)\n",
        "attention_scores= inputs @ inputs.T\n",
        "attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0va7xWnCd6nM",
        "outputId": "9b797d9a-012f-4c4b-ff2d-0c1d5859d081"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.7532, -0.1701, -0.6722,  4.3272],\n",
              "        [-0.1701,  5.1665, -1.1530, -2.0556],\n",
              "        [-0.6722, -1.1530,  4.9508,  1.6680],\n",
              "        [ 4.3272, -2.0556,  1.6680, 10.7603]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights= torch.softmax(attention_scores, dim=-1)\n",
        "attention_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfnnRJqsgawK",
        "outputId": "48681250-35e4-4f7b-e21b-616af0036f69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.0350e-01, 2.1505e-03, 1.3016e-03, 1.9305e-01],\n",
              "        [4.7775e-03, 9.9271e-01, 1.7878e-03, 7.2501e-04],\n",
              "        [3.4635e-03, 2.1414e-03, 9.5843e-01, 3.5964e-02],\n",
              "        [1.6046e-03, 2.7126e-06, 1.1234e-04, 9.9828e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El0iywpzhX5f",
        "outputId": "7c3aab23-b7ee-4b04-cfed-07c3240f2504"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vectors=attention_weights @ inputs\n",
        "context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWPvkZ41hd_S",
        "outputId": "359e77f1-1b89-452b-ff57-6a6962906c71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9024, -1.0648, -0.1079,  0.8976,  0.2203,  1.0214, -0.9928,  0.7824],\n",
              "        [ 0.9018,  0.0369,  0.9222,  0.4704, -0.8568,  0.4583,  1.3251, -0.7045],\n",
              "        [-0.5278, -1.7007, -0.7114, -0.0654,  0.0182, -0.8376,  0.0758, -0.5287],\n",
              "        [ 1.2317, -1.4538, -2.0181,  0.6829,  0.7032,  0.6569, -0.2543,  1.2559]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}